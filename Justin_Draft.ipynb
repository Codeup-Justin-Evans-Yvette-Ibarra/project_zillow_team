{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f137d882",
   "metadata": {},
   "source": [
    "# 2017 Tax Assesed Home Value Prediction Models\n",
    "\n",
    "# Group 4\n",
    "* Justin Evans\n",
    "* Yvette Ibarra\n",
    "\n",
    "# Project Overview:\n",
    "This project has been tasked with collecting, cleaning and analyzing Zillow data from 2017 in order to improve a previous prediction model that was designed to predict the Tax Assessed Home Value for Single Family Properties based on available realestate data.\n",
    "\n",
    "# Goals: \n",
    "* Predict property tax assessed values of Single Family Properties\n",
    "* Outperform existing logerror model\n",
    "* Recommend improvements for a more accurate model\n",
    "* Define relevant fips codes for our data\n",
    "\n",
    "# Reproduction of this Data:\n",
    "* Can be accomplished using a local ```env.py``` containing ```user, password, host``` information for access to the Codeup SQL database server.\n",
    "* All other step by step instructions can be found by reading the below Jupyter Notebook files located in my [proj-2_zillow](https://github.com/QMCBT-JustinEvans/project-2_zillow.git) github repository.\n",
    "    * 01_wrangle\n",
    "    * 02_explore\n",
    "    * 03_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5e201",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Decision Tree and Model Evaluation Imports\n",
    "import sklearn.preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "\n",
    "# import sklearn.metrics\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import plot_confusion_matrix \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "# import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# assists with processor speed\n",
    "#import matplotlib as mpl\n",
    "#mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Increases Display Resolution for Graphs \n",
    "# Only works inside notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# import Environment and Helper Files\n",
    "from env import user, password, host\n",
    "import QMCBT_00_quicktips as qt\n",
    "#import QMCBT_01_acquire as acquire\n",
    "#import QMCBT_02_prepare as prepare\n",
    "#import QMCBT_03_explore as explore\n",
    "#import QMCBT_04_modeling as model\n",
    "#import QMCBT_05_evaluate as evaluate\n",
    "import QMCBT_explore_evaluate as ee\n",
    "import wrangle as w\n",
    "\n",
    "# set constants\n",
    "Î± = 0.05\n",
    "alpha = 0.05\n",
    "np.random.seed(123)\n",
    "#random_state=123\n",
    "\n",
    "# Turns off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Left Align Tables in Jupyter Notebook\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3df592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows import reload without needing to clear kernel and rerun\n",
    "# reload(packagename) \n",
    "from importlib import reload\n",
    "\n",
    "# This code refreshes all of my helper files (w/o the need to stop the kernel)\n",
    "reload(qt)\n",
    "#reload(acquire)\n",
    "#reload(prepare)\n",
    "#reload(explore)\n",
    "#reload(model)\n",
    "#reload(evaluate)\n",
    "reload(ee)\n",
    "reload(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa01ee8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "    \n",
    "## Data Acquisition: \n",
    "Data is collected from the codeup cloud database with an appropriate SQL query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497ac36",
   "metadata": {},
   "source": [
    "# Acquire\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* ```zillow``` data from Codeup SQL database was used for this project.\n",
    "* The data was initially pulled on 15-NOV-2022.\n",
    "* The initial DataFrame contained 52,441 records with 69 features  \n",
    "    (69 columns and 52,441 rows) before cleaning & preparation.\n",
    "* Each row represents a Single Family Property record with a Tax Asessment date within 2017.\n",
    "* Each column represents a feature provided by Zillow or an informational element about the Property.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31449a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = w.get_zillow_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c8562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02e799",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Prepare Actions:**\n",
    "\n",
    "* **Whitespace:** Removed 52,441 Whitespace characters.\n",
    "* **REFORMAT:** Reformatted 13 columns containing 596,382 NaN entries to 0.\n",
    "* **CONVERT dtypes:** Convert dtypes to accurately reflect data contained within Feature.\n",
    "* **FEATURE ENGINEER:** Use Yearbuilt to create Age Feature, Drop yearbuilt for redundancy; create Feature to show ratio of Bathrooms to Bedrooms.\n",
    "* **fips CONVERSION:** Use fips master list to convert fips to county and state, Drop state for redundancy.\n",
    "* **PIVOT:** Pivot the resulting county column from fips conversion to 3 catagorical features. \n",
    "* **DROP:** Dropped 27 Columns unecessary to data prediction (ie.. index and redundant features).\n",
    "* **REPLACE:** Replaced conditional values in 2 columns to transform into categorical features.\n",
    "* **RENAME:** Columns for Human readability.    \n",
    "* **REORDER:** Rearange order of columns for human readability.   \n",
    "* **DROP 2:** Drop Location Reference Columns unsuitable for use with ML without categorical translation.\n",
    "* **CACHE:** Write cleaned DataFrame into a new csv file ('zillow_2017_cleaned.csv').  \n",
    "* **ENCODED:** No encoding required.\n",
    "* **MELT:** No melts needed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f0d4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "    \n",
    "# Summary of Data Cleansing\n",
    "* Cleaning the data resulted in less than 6% overall record loss\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4bc79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "* <b>DROP NaN COLUMNS:</b> 39 features each containing over 30% NaN were dropped; <b>resulting in no record loss</b>.\n",
    "  \n",
    "* <b>DROP NaN ROWS:</b> 1,768 records containing NaN across 13 features were dropped; <b>resulting in only 3% record loss</b>.\n",
    "\n",
    "* <b>OUTLIERS:</b> Aproximately 3,000 outliers were filtered out in an attempt to more accurately align with realistic expectations of a Single Family Residence; <b>resulting in less than a 6% decrease in overall records</b>.\n",
    "\n",
    "* <b>IMPUTE:</b> No data was imputed</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493261e6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "    \n",
    "## Data Prep:\n",
    "* Column data types are appropriate for the data they contain\n",
    "* Missing values are investigated and handled\n",
    "* Outliers are investigated and handled\n",
    "    * BED and Bath >0 <8  \n",
    "    * SQFT >400 <10_000  \n",
    "    * Tax Value < 2 million  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04173ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = w.zillow_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe8018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443e083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399b6fb",
   "metadata": {},
   "source": [
    "# Split\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "* **SPLIT:** train, validate and test (approx. 60/20/20), stratifying on target of 'churn'\n",
    "* **Xy SPLIT:** split each DataFrame (train, validate, test) into X (selected features) and y (target) \n",
    "* **SCALED:** no scaling was conducted\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, X_train, X_validate, X_test, y_train, y_validate, y_test = w.split(df, 'log_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d697b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prcnt = round((train.shape[0] / df.shape[0]), 2)*100\n",
    "validate_prcnt = round((X_validate.shape[0] / df.shape[0]), 2)*100\n",
    "test_prcnt = round((X_test.shape[0] / df.shape[0]), 2)*100\n",
    "target = 'log_error'\n",
    "\n",
    "print(f'Prepared df: {df.shape}')\n",
    "print()\n",
    "print(f'      Train: {train.shape} - {train_prcnt}%')\n",
    "print(f'   Validate: {X_validate.shape} - {validate_prcnt}%')\n",
    "print(f'       Test: {X_test.shape} - {test_prcnt}%')\n",
    "print()\n",
    "print()\n",
    "print(f'   X_train: {X_train.shape}')\n",
    "print(f'   y_train: {y_train.shape}     Index({target})')\n",
    "print()\n",
    "print(f'X_validate: {X_validate.shape}')\n",
    "print(f'y_validate: {y_validate.shape}     Index({target})')\n",
    "print()\n",
    "print(f'    X_test: {X_test.shape}')\n",
    "print(f'    y_test: {y_test.shape}     Index({target})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d4473",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "    \n",
    "## Exploration: \n",
    "* the interaction between independent variables and the target variable is explored using visualization and statistical testing\n",
    "* Clustering is used to explore the data. \n",
    "* A conclusion, supported by statistical testing and visualization, is drawn on whether or not the clusters are helpful/useful. \n",
    "* At least 3 combinations of features for clustering should be tried.\n",
    "    * 4 Questions\n",
    "    * 2 stats\n",
    "    * 5-7 graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b5299",
   "metadata": {},
   "source": [
    " **A. Set Hypothesis**  \n",
    "  \n",
    " * One Tail (```<= | >```) or Two Tails (```== | !=```)?\\  \n",
    "  **two_tail (gender, been_manager)**  \n",
    "  \n",
    "  \n",
    " * One Sample or Two Samples?\\  \n",
    "  **two_sample (gender, been_manager)**  \n",
    "  \n",
    "  \n",
    " * Continuous or Discreat?\\  \n",
    "  **Discreat (gender) vs Discreat (been_manager) = $Chi^2$**  \n",
    "      * T-Test = ```Discreat``` vs ```Continuous```  \n",
    "      * Pearsonâs = ```Continuous``` vs ```Continuous``` (linear)  \n",
    "      * $Chi^2$ = ```Discreat``` vs ```Discreat```  \n",
    "  \n",
    "  \n",
    " * $ð»_0$: The opposite of what I am trying to prove\\  \n",
    "  **$H_{0}$: The employee gender is **NOT** ```dependent``` on whether the employee has been a manager**\\  \n",
    "  ```employees.gender ``` != ```employees.been_manager```  \n",
    "  \n",
    "  \n",
    " * $ð»_ð$: What am I trying to prove\\  \n",
    "  **$H_{a}$: The employee gender is ```dependent``` on whether the employee has been a manager**\\  \n",
    "  ```employees.gender ``` == ```employees.been_manager```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbeb8c1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "## Q1. Is there a relationship between censustractandblock and logerror?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de770b7",
   "metadata": {},
   "source": [
    " **Set Hypothesis**  \n",
    "  \n",
    " * One Tail (```<= | >```) or Two Tails (```== | !=```)?\\  \n",
    "  **two_tail (censustractandblock, log_error)**  \n",
    "  \n",
    "  \n",
    " * One Sample or Two Samples?\\  \n",
    "  **two_sample (censustractandblock, log_error)**  \n",
    "  \n",
    "  \n",
    " * Continuous or Discreat?\\  \n",
    "  **Continuous (censustractandblock) vs Continuous (log_error) = Pearsonâs**  \n",
    "      * T-Test = ```Discreat``` vs ```Continuous```  \n",
    "      * Pearsonâs = ```Continuous``` vs ```Continuous``` (linear)  \n",
    "      * $Chi^2$ = ```Discreat``` vs ```Discreat```  \n",
    "  \n",
    "  \n",
    " * $ð»_0$: The opposite of what I am trying to prove  \n",
    " * $H_{0}$:  There is **NOT** a significant ```relationship``` between the Census Neighborhood and our target log_error  \n",
    "  ```censustractandblock``` != ```log_error```  \n",
    "  \n",
    "  \n",
    " * $ð»_ð$: What am I trying to prove  \n",
    " * $H_{a}$: There is a significant ```relationship``` between the Census Neighborhood and our target log_error  \n",
    "  ```censustractandblock``` == ```log_error``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd32899",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='censustractandblock', y='log_error', data=train, line_kws={'color': 'red'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a22d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(train.corr(method='pearson')[['censustractandblock']].sort_values(by='censustractandblock', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with Census Neighborhood', fontdict={'fontsize':18}, pad=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ff85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots() # Create the figure and axes object\n",
    "\n",
    "#### Plot the first x and y axes:\n",
    "train.plot(x = 'censustractandblock', y = 'log_error', ax = ax) \n",
    "#### Plot the second x and y axes. By secondary_y = True a second y-axis is requested:\n",
    "#### (see https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html for details)\n",
    "#df.plot(x = 'lotsizesquarefeet', y = 'taxvaluedollarcnt', ax = ax, secondary_y = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d109e",
   "metadata": {},
   "source": [
    "## Test Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbb576",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "\n",
    "# Pearson's R\n",
    "**Compute test statistic and probability (r & p_value)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354aeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r, p_val = stats.pearsonr(train.censustractandblock,\n",
    "                      train.log_error)\n",
    "r, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d015703",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p_val < Î±:\n",
    "    print('Reject the null hypothesis')\n",
    "else:\n",
    "    print('Fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9a7e7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "    \n",
    "The Census Neighborhood has a ```significant Relationhip``` with our target log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94ed5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea862a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791bbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3fd3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1de1285e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.column_stats(df, 'censustractandblock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_scale = ['fips',\n",
    "                    'longitude', \n",
    "                    'latitude',\n",
    "                    'zipcode', \n",
    "                    'censustractandblock']\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_scaled, validate_scaled, test_scaled = w.scale_data(train,\n",
    "                                                           X_validate, \n",
    "                                                           X_test, \n",
    "                                                           columns_to_scale,\n",
    "                                                           scaler,\n",
    "                                                           return_scaler = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c545d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "\n",
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f686c4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = df[['fips',\n",
    "                        'county',\n",
    "                        'la_county', \n",
    "                        'orange_county', \n",
    "                        'ventura_county',\n",
    "                        'longitude', \n",
    "                        'latitude',\n",
    "                        'zipcode', \n",
    "                        'regionidcounty', \n",
    "                        'rawcensustractandblock', \n",
    "                        'censustractandblock']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea80ebb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "    \n",
    "## Modeling: \n",
    "At least 4 different models are created and their performance is compared. One model is the distinct combination of algorithm, hyperparameters, and features.\n",
    "* Establish Baseline of logerror\n",
    "* Establish Metric for Modeling (RMSE, R^2)\n",
    "* Evaluation DataFrame\n",
    "* Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383bf0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b9d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bf715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4b22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c10ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa19c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5850c65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96fd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cb6a590",
   "metadata": {},
   "source": [
    "# COLUMN MANAGEMENT\n",
    "\n",
    "\n",
    "# REARANGE\n",
    "df = df[['parcelid',\n",
    "         'bedroomcnt',\n",
    "         'bathroomcnt', \n",
    "         'calculatedbathnbr', \n",
    "         'fullbathcnt',\n",
    "         'age', \n",
    "         'yearbuilt', \n",
    "         'basementsqft', \n",
    "         'decktypeid', \n",
    "         'fireplacecnt', \n",
    "         'garagecarcnt', \n",
    "         'hashottuborspa', \n",
    "         'poolcnt', \n",
    "         'optional_features', \n",
    "         'taxdelinquencyyear', \n",
    "         'fips',\n",
    "         'state', \n",
    "         'name',\n",
    "         'Los Angeles County', \n",
    "         'Orange County', \n",
    "         'Ventura County'\n",
    "         'longitude', \n",
    "         'latitude',\n",
    "         'regionidzip', \n",
    "         'regionidcounty', \n",
    "         'rawcensustractandblock', \n",
    "         'censustractandblock', \n",
    "         'calculatedfinishedsquarefeet',\n",
    "         'lotsizesquarefeet', \n",
    "         'structuretaxvaluedollarcnt',\n",
    "         'taxvaluedollarcnt', \n",
    "         'landtaxvaluedollarcnt',\n",
    "         'taxamount', \n",
    "         'log_error',]]\n",
    "\n",
    "## Continuous Categorical Counts\n",
    "'parcelid', \n",
    "'bedroomcnt',\n",
    "'bathroomcnt', \n",
    "'calculatedbathnbr', \n",
    "'fullbathcnt',\n",
    "'age', \n",
    "'yearbuilt', \n",
    "\n",
    "## Categorical Binary\n",
    "'basementsqft', \n",
    "'decktypeid', \n",
    "'fireplacecnt', \n",
    "'garagecarcnt', \n",
    "'hashottuborspa', \n",
    "'poolcnt', \n",
    "'taxdelinquencyyear', \n",
    "'optional_features', \n",
    "\n",
    "## Location\n",
    "'fips',\n",
    "'state', \n",
    "'name',\n",
    "'Los Angeles County', \n",
    "'Orange County', \n",
    "'Ventura County'\n",
    "'longitude', \n",
    "'latitude',\n",
    "'regionidzip', \n",
    "'regionidcounty', \n",
    "'rawcensustractandblock', \n",
    "'censustractandblock', \n",
    "\n",
    "## Size\n",
    "'calculatedfinishedsquarefeet',\n",
    "'lotsizesquarefeet', \n",
    "\n",
    "## Value\n",
    "'structuretaxvaluedollarcnt',\n",
    "'taxvaluedollarcnt', \n",
    "'landtaxvaluedollarcnt',\n",
    "'taxamount', \n",
    "\n",
    "## Target\n",
    "'log_error',\n",
    "\n",
    "# DROP\n",
    "# These columns were dropped by not adding them into the rearrange assignment\n",
    "'id', \n",
    "'garagetotalsqft', \n",
    "'poolsizesum',\n",
    "'pooltypeid10', \n",
    "'pooltypeid2', \n",
    "'pooltypeid7',\n",
    "'propertycountylandusecode', \n",
    "'propertylandusetypeid',\n",
    "'roomcnt',\n",
    "'finishedsquarefeet12',\n",
    "'numberofstories', \n",
    "'assessmentyear', \n",
    "'transaction_date', \n",
    "'land_use', \n",
    "\n",
    "# RENAME\n",
    "#### Rename Binary Categoricals\n",
    "df.rename(columns = {'hashottuborspa': 'has_hottuborspa',\n",
    "                     'taxdelinquencyyear': 'has_taxdelinquency', \n",
    "                     'basementsqft': 'has_basement', \n",
    "                     'poolcnt': 'has_pool', \n",
    "                     'decktypeid': 'has_deck'\n",
    "                     'fireplacecnt': 'has_fireplace'\n",
    "                     'garagecarcnt': 'has_garage'\n",
    "                     'Las Angele County': 'las_angeles'\n",
    "                     'Ventura County': 'ventura'\n",
    "                     'Orange County': 'orange'}\n",
    "          , inplace = True)\n",
    "\n",
    "#### Rename  Human Readable\n",
    "df.rename(columns = {'name': 'county'\n",
    "                     'bedroomcnt': 'bedrooms'\n",
    "                     'bathroomcnt': 'bathrooms'\n",
    "                     'structuretaxvaluedollarcnt': 'tax_value_bldg'\n",
    "                     'taxvaluedollarcnt': 'tax_value'\n",
    "                     'landtaxvaluedollarcnt': 'tax_value_land'\n",
    "                     'regionidzip': 'zipcode'\n",
    "                     'lotsizesquarefeet': 'lot_sqft'\n",
    "                     'calculatedfinishedsquarefeet': 'sqft'}\n",
    "          , inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39154d",
   "metadata": {},
   "source": [
    "# COPY PASTE CODE TO PLAY WITH\n",
    "\n",
    "\n",
    "\n",
    "####################################### NULL VALUES ############################################\n",
    "def null_counter(df):\n",
    "    ''' null_counter takes in a dataframe anc calculates the percent and amount of null cells in each column and row\n",
    "    returns a dataframe with the results'''\n",
    "    # name of dataframe names\n",
    "    new_columns = ['name', 'num_rows_missing', 'pct_rows_missing']\n",
    "    \n",
    "    # create data frame\n",
    "    new_df = pd.DataFrame(columns = new_columns)\n",
    "   \n",
    "    # for loop to calculate missing /percent by columns\n",
    "    for col in list(df.columns):\n",
    "        num_missing = df[col].isna().sum()\n",
    "        pct_missing = num_missing / df.shape[0]\n",
    "        \n",
    "        # create data frame\n",
    "        add_df = pd.DataFrame([{'name': col,\n",
    "                               'num_rows_missing': num_missing,\n",
    "                               'pct_rows_missing': pct_missing}])\n",
    "       \n",
    "        # concat and index by row by seting axis to 0   \n",
    "        new_df = pd.concat([new_df, add_df], axis = 0)\n",
    "        \n",
    "    # sets the index name\n",
    "    new_df.set_index('name', inplace = True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def null_dropper(df,prop_required_column,prop_required_row):\n",
    "\n",
    "    ''' null_dropper takes in a dataframe a percent of required columns and rows to keep columns.\n",
    "    all columns and rows outside of the null threshold will be dropped\n",
    "    returns a clean dataframe dropped nulls'''\n",
    "    \n",
    "    # this is a decimal = 1- decimal\n",
    "    prop_null_column = 1-prop_required_column\n",
    "    \n",
    "    # for columns, check null percentage and drop if a certain proportion is null (set by definition)\n",
    "    for col in list(df.columns):\n",
    "        null_sum = df[col].isna().sum()\n",
    "        null_pct = null_sum / df.shape[0]\n",
    "        \n",
    "        if null_pct > prop_null_column:\n",
    "            df.drop(columns = col, inplace = True)\n",
    "    \n",
    "    # for rows, drop if a certain proportion is null. (set by definition)\n",
    "    row_threshold = int(prop_required_row * df.shape[1])\n",
    "    \n",
    "    df.dropna(axis = 0, thresh=row_threshold, inplace = True)\n",
    "    \n",
    "    return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
